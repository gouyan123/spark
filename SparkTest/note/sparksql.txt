SparkSQL 2.x使用DataSet，DataFrame，不使用RDD了；DataFrame = RDD + Schema；DataSet是对RDD的封装，可以转换为DataFrame，只有一列；DataFrame与DataSet关系？？？
---
sparksql案例，都在SparkTest项目中：
创建 cn/edu360/day6/SQLDemo1 类，使用SparkSQL 1.x 的 API 操作sql；
---
创建 cn.edu360.day6.SQLTest1 类，使用SparkSQL 2.x 的 API 操作sql；
---
创建 cn.edu360.day6.SQLWordCount类，使用 Dataset的 sql写 WordCount；
DataSet只有一列；里面有执行计划，会从多个可行方案中选一个最优的；
show()方法本质，将计算任务下发到集群，集群各节点计算完成后，将结果收集回来
---
创建 cn.edu360.day6.DataSetWordCount，使用 Dataset的 API写 WordCount；
DataSet只有一列；里面有执行计划，会从多个可行方案中选一个最优的；
show()方法本质，将计算任务下发到集群，集群各节点计算完成后，将结果收集回来
---
小结：SparkSQL 2.x和Dataset必须会；DataSet里面有执行计划，会从多个可行方案中选一个最优的；
---
创建 cn.edu360.day7.JoinTest，使用 DataSet进行 join操作；
------------------------------------------------------
spark读取多种类型数据源：spark.read.format("jdbc").options()
创建 cn.edu360.day7.JdbcDataSource，读取 jdbc数据；
---
创建 cn.edu360.day7.JsonDataSource，读取 json数据；
---
创建 cn.edu360.day7.ParquetDataSource，读取 parquet数据 ***重点***；
parquet是一种特殊的文件格式，不仅用parquet存储，还用snappy压缩了，打开是类似二进制的文件；parquet里面不仅保存 数据，还保存schema和元数据；
100-600字节处保存 id，600-900字节处保存 name，900-1200字节处保存 age，1200-1500字节处保存 fv；
写一条查询语句 select name，fv from users;然后 sparkSession读取 parquet文件生成 DataFrame，DataFrame注册临时表，然后执行这个 sql查询语句；重点：sparkSession
真正读取 parquet文件时，只读取 select语句中要的 name，fv字段；
如图 parquet文件详解.png；
---