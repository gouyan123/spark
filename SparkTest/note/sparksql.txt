SparkSQL 2.x使用DataSet，DataFrame，不使用RDD了；DataFrame = RDD + Schema；DataSet是对RDD的封装，可以转换为DataFrame，只有一列；DataFrame与DataSet关系？？？
---
sparksql案例，都在SparkTest项目中：
创建 cn/edu360/day6/SQLDemo1 类，使用SparkSQL 1.x 的 API 操作sql；
---
创建 cn.edu360.day6.SQLTest1 类，使用SparkSQL 2.x 的 API 操作sql；
---
创建 cn.edu360.day6.SQLWordCount类，使用 Dataset的 sql写 WordCount；
DataSet只有一列；里面有执行计划，会从多个可行方案中选一个最优的；
show()方法本质，将计算任务下发到集群，集群各节点计算完成后，将结果收集回来
---
创建 cn.edu360.day6.DataSetWordCount，使用 Dataset的 API写 WordCount；
DataSet只有一列；里面有执行计划，会从多个可行方案中选一个最优的；
show()方法本质，将计算任务下发到集群，集群各节点计算完成后，将结果收集回来
---
小结：SparkSQL 2.x和Dataset必须会；DataSet里面有执行计划，会从多个可行方案中选一个最优的；
---
创建 cn.edu360.day7.JoinTest，使用 DataSet进行 join操作；
------------------------------------------------------
spark读取多种类型数据源：spark.read.format("jdbc").options()
创建 cn.edu360.day7.JdbcDataSource，读取 jdbc数据；
---
创建 cn.edu360.day7.JsonDataSource，读取 json数据；
---
创建 cn.edu360.day7.ParquetDataSource，读取 parquet数据 ***重点***；
parquet是一种特殊的文件格式，不仅用parquet存储，还用snappy压缩了，打开是类似二进制的文件；parquet里面不仅保存 数据，还保存schema和元数据；
100-600字节处保存 id，600-900字节处保存 name，900-1200字节处保存 age，1200-1500字节处保存 fv；
写一条查询语句 select name，fv from users;然后 sparkSession读取 parquet文件生成 DataFrame，DataFrame注册临时表，然后执行这个 sql查询语句；重点：sparkSession
真正读取 parquet文件时，只读取 select语句中要的 name，fv字段；
如图 parquet文件详解.png；
---
创建 cn.edu360.day9.SteamingWordCount，使用 spark streaming实现 实时wordcount；
---
创建 cn.edu360.day9.KafkaWordCount，使用 sparkstreaming从kafka实时读取数据，进行wordcount；程序Exit再启动后，不会重新消费之前的数据，因为zookeeper中记录了
偏移量；正常情况下，应该手动维护 kafka偏移量；
---
创建 cn.edu360.day9.StatefulKafkaWordCount，实现 更新状态累加的WordCount；存在问题，程序退出后，中间结果都丢了；用这这个 updateFunc方法比较少，一般把数据
累计结果保存到 redis里面；
---
写一个生产级别的程序，会记录偏移量，保证数据只被消费一次；使用 Direct Approach方式，RDD的一个分区直接连到 kafka的一个分区，通过迭代器拉取kafka分区数据，来一条
拉一条，一直在不停处理，但是一段时间作为一个批次；
---
创建 cn.edu360.day9.KafkaDirectWordCount，使用 直连模式读取kafka数据；使用 组名 + topic共同定义偏移量；
这个代码不用会写，理解之后，会改就行；
---
创建 cn.edu360.day9.KafkaDirectWordCountV2，对直连模式KafkaDirectWordCount进行改造；
---
创建 cn.edu360.day10.OrderCount，统计订单，直接 拷贝KafkaDirectWordCountV2，修改即可，主要在kafkaStream.foreachRDD { kafkaRDD =>...}里面写，
kafkaStream.foreachRDD表示每隔一段时间生成一个批次的RDD；

---